---
title: ''
editor:
  render-on-save: true
format:
  pdf:
    documentclass: exam
    papersize: letter
    fontsize: 10pt
    include-in-header:
      - ./my-header.tex
      - ./my-macros.tex
    toc: false
    number-sections: true
    colorlinks: false
---


\noindent\makebox[0.75\textwidth]{Name:\enspace\hrulefill}

\vspace{5mm}

\begin{center}
    \LARGE{\textbf{In-class midterm, EC421}}
    \\ \large{145 points possible}
\end{center}

\vspace{5mm}

```{r, setup, include=FALSE}
# Packages
library(pacman)
p_load(fastverse, extrafont, latex2exp, ggplot2, scales)
```

## True or false (75 points; 30 questions)

**Note** In this section, select the correct answer (true or false). You do not need to explain your answer.

\begin{questions}

\tf{T} If the disturbance correlates with a regressor, then exogeneity is violated.

\tf{F} When OLS is biased for the standard errors, it is also biased for the coefficients.

\tf{F} The \textit{linearity} assumption in OLS prohibits models from including squared terms like $x_{i}^{2}$.

\tf{F} Omitted-variable bias occurs when an omitted variable correlates with an included regressor.

\tf{T} Adding more explanatory variables will always increase $R^2$.

\tf{F} Heteroskedasticity makes OLS estimates of coefficients biased.

\tf{T} Weighted least squares gives more weight to observations with smaller variances in their disturbances.

\tf{T} Residuals are observable; disturbances are not.

\tf{F} If \( \hat{\beta}_1 = 0.3 \) in a log-log model, a 1\% increase in \( x \) increases \( y \) by 30\%.

\tf{F} A p-value of 0.99 indicates a high degree of statistical significance.

\tf{T} Consistency describes the behavior of an estimator as sample size grows.

\tf{F} Heteroskedasticity-robust standard errors are biased in the presence of homoskedasticity.

\tf{F} Correlated disturbances violate the exogeneity assumption.

\tf{T} The Goldfeld-Quandt test compares error variances across two subsets of the data.

\tf{T} A coefficient on an interaction term captures how the effect of one variable depends on the level of another.

\tf{T} Functional-form misspecification can lead to heteroskedasticity.

\tf{F} Homoskedasticity means \( \text{Var}(u_i) \neq \text{Var}(u_j) \) for two individuals \( i \) and \( j \).

\tf{T} The White test is better than the Goldfeld-Quandt test at detecting general heteroskedasticity.

\tf{T} The sum of squared residuals directly influences the magnitude of the standard errors.

\tf{F} If \( \text{Cov}(x_i, u_i) \neq 0 \), then OLS is still unbiased but inconsistent.

\tf{T} In the regression \( \log(y_i) = \beta_0 + \beta_1 x_i + u_i \), the effect of \( x \) on \( y \) is percent-based.

\tf{T} A violation of exogeneity implies biased OLS coefficient estimates.

\tf{F} OLS minimizes the $\sum_i e_i$, where \( e_i \) is the residual.

\tf{T} The presence of heteroskedasticity invalidates standard OLS hypothesis tests.

\tf{T} For an estimator with bias \( 1/n \), the estimator may still be consistent.

\tf{F} The assumption \( \mathbb{E}[u_i^2|X_i] = \sigma^2 \) is necessary for OLS to be unbiased for the coefficients.

\tf{F} The p-value tells us the probability that our hypothesis is true.

\tf{F} WLS is less efficient that OLS in the presence of heteroskedasticity.

\tf{F} For the regression model
$$\text{Income}_i = \beta_0 + \beta_1 \text{Health}_i + u_i,$$
the White test for heteroskedasticity would run the following regression:
$$\text{Income}_i^2 = \beta_0 + \beta_1 \text{Health}_i + \beta_2 \text{Health}_i^2 + u_i.$$

\tf{T} Heteroskedasticity-robust standard errors and `plain' OLS standard errors will always use the same coefficient estimates.

\end{questions}

\newpage

## Multiple choice (20 points; 5 questions)

**Note** In this section, check ($\checkmark$ or $\times$) \textbf{all} correct answers. You do not need to explain your answer.

\begin{questions}

\setcounter{question}{30}

\mc \\ Which of the following assumptions are ``classic'' regression assumptions?

\vspace{\stretch{0}}
\begin{oneparcheckboxes} 
  \CorrectChoice $E[u_i | X_i] = 0$
  \choice $\text{Var}(u_i) = 0$ for all $i$
  \choice $\text{Cov}(u_i, u_j) = \sigma$
  \CorrectChoice $\text{Var}(X) > 0$
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ Which of the relationships imply OLS is biased for estimating the coefficients in a regression model?

\vspace{\stretch{0}}
\begin{oneparcheckboxes} 
  \choice $E[u_i | X_i] = 0$
  \choice $\text{Var}(u_i) \neq \text{Var}(u_j)$
  \CorrectChoice $E\left[\hat{\beta}\right] \neq \beta$
  \choice $\text{Cov}(u_i, u_j) = 0$
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ In the presence of heteroskedasticity, which of the following statements are true?

\vspace{\stretch{0}}
\begin{oneparcheckboxes}
  \CorrectChoice OLS is unbiased for the coefficients.
  \CorrectChoice Standard OLS confidence intervals are biased. \newline
  \choice OLS is unbiased for the standard errors.
  \CorrectChoice OLS is less efficient than WLS.
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ Imagine you are in a setting where you believe the disturbance is heteroskedastic. What are your `options' for estimating the model that will yield believable estimates?

\vspace{\stretch{0}}
\begin{oneparcheckboxes} 
  \CorrectChoice Use het.-robust standard errors.
  \CorrectChoice Estimate the model using WLS. \newline
  \CorrectChoice Check the specification.
  \choice Look for omitted variables.
\end{oneparcheckboxes} 
\vspace{\stretch{2}}

\mc \\ Which of the following will generally make our standard errors smaller?

\vspace{\stretch{0}}
\begin{oneparcheckboxes} 
  \CorrectChoice Adding additional regressors to the model.
  \choice Omitted variable bias. \newline
  \CorrectChoice Ignoring correlated disturbances.
  \choice Subtracting the mean from the outcome variable.
\end{oneparcheckboxes} 
\vspace{\stretch{2}}


\end{questions}

## Short answer (50 points; 10 questions)

\textbf{Note} In this section, briefly answer the questions/prompts in 1--3 short (and complete) sentences. \newline We will deduct points for excessively long answers.

\begin{questions}

\setcounter{question}{35}

\short Define the concept of a \textit{standard error}.

\begin{solution}
  The standard error is the standard deviation of an estimator's sampling distribution. It quantifies the amount of variability in the estimate due to sampling.
\end{solution}

\vspace{\stretch{1}}

\short Explain how OLS defines the ``best-fit'' line.

\begin{solution}
  OLS defines the best-fit line by minimizing the sum of squared residuals, where the \textit{residuals} the differences between the observed values and the predicted values (the line).
\end{solution}

\vspace{\stretch{1}}

\short Explain the difference between a \textit{residual} and a \textit{disturbance}.

\begin{solution}
  A residual is the difference between the observed value and the sample line (observable), while a disturbance is the difference between the observed value and the population line (unobservable). The disturbance is the error term in the population model, while the residual is the error term in the sample model.
\end{solution}

\vspace{\stretch{1}} 

\newpage

\short We showed in class that the probability limit of the OLS estimator (for the slope coefficient in a simple linear regression) is
$$
  \text{plim}\, \hat{\beta}_1 = \beta_1  + \frac{\text{Cov}(x,u)}{\text{Var}(x)}
$$
Using this formula, explain how our OLS assumptions imply that the OLS estimator is consistent (when the assumptions are satisfied).

\begin{solution}
  The assumption of exogeneity implies that the covariance between the regressor and the disturbance is zero, \ie \( \text{Cov}(x,u) = 0 \).

  We also assume that the variance of the regressor is non-zero, \ie \( \text{Var}(x) > 0 \).
  
  Together, these two assumptions imply the second term in the equation above is zero.
  
  Thus, the probability limit of the OLS estimator is equal to the true population parameter \( \beta_1 \), which implies that the OLS estimator is consistent.
\end{solution}

\vspace{\stretch{1}}

\short Define the two requirements for a variable to cause \textit{omitted-variable bias}.

\begin{solution}
  The two requirements for a variable to cause omitted-variable bias are:
  \begin{enumerate}
    \item the omitted variable must correlate with the included regressor;
    \item the omitted variable must affect the dependent variable.
  \end{enumerate}
\end{solution}

\vspace{\stretch{1}} 

\short How do we quantify the uncertainty behind our OLS estimates?

\begin{solution}
  We quantify the uncertainty behind our OLS estimates using standard errors, which we use to construct confidence intervals and conduct hypothesis tests.
\end{solution}

\vspace{\stretch{1}}

\newpage

The questions on this page refer to the regression model below.
$$
  \text{Health}_i = \beta_0 + \beta_1 \text{Income}_i + \beta_2 \text{Hispanic}_i + \beta_3 \text{Income}_i \times \text{Hispanic}_i + u_i
$$
where $\text{Health}_i$ is an index that runs between 0 and 100; $\text{Income}_i$ measured in thousands of dollars; $\text{Hispanic}_i$ is a binary (indicator) variable that equals $1$ if individual $i$ is Hispanic and 0 otherwise.

Suppose $\beta_0 = 10$, $\beta_1 = 1$, $\beta_2 = -5$, and $\beta_3 = 0.5$. 

\short Interpret $\beta_1$ and $\beta_3$ in the context of the model above.

\begin{solution}
  The coefficient \( \beta_1 = 1 \) tells us the effect of income on health for non-Hispanic individuals: for a \$1,000 increase in income, we expect the health index increases by 1 point, holding all else constant.

  The coefficient \( \beta_3 = 0.5 \) tells us how the effect of income on health differs between Hispanic and non-Hispanic individuals. Specifically, the value here indicates that for a \$1,000 increase in income, the health index increases an additional 0.5 points for Hispanic individuals, relative to non-Hispanic individuals.
\end{solution}

\vspace{\stretch{1}}

\short What is the expected value of health for a Hispanic individual with an income of \$50,000?

\begin{solution}
  To find the expected value of health for a Hispanic individual with an income of \$50,000, we plug in the values into the regression equation:
  \[
    \text{Health}_i = 10 + 1 \times 50 + (-5) \times 1 + 0.5 \times 50 \times 1 = 10 + 50 - 5 + 25 = 80
  \]
  
  Thus, the expected value of health for a Hispanic individual with an income of \$50,000 is 80.
\end{solution}

\vspace{\stretch{1}}

\newpage

\short Draw (approximately) the graph of the two lines implied by the preceding regression model. \newline Don't worry about making it perfect---but do make sure to label the axes and get the general idea right.

\begin{solution}
  The big thing here is to notice that the line for Hispanic individuals starts lower (has a lower intercept) and has a steeper slope than the line for non-Hispanic individuals. 
\end{solution}

\end{questions}

```{r, interaction}
#| echo: false
#| fig.height: 3 

# Load packages
pacman::p_load(tibble, ggplot2, scales)
# Make data
df = tibble(
  x = c(0, 100),
  y1 = 10 + 1 * x,
  y2 = 10 + 1 * x - 5 + 0.5 * x
)
# Plot
ggplot(df, aes(x = x)) +
  geom_line(aes(y = y1), color = 'grey80', linewidth = 1) +
  geom_line(aes(y = y2), color = 'purple', linewidth = 1) +
  labs(
    x = "Income (in thousands)",
    y = "Health index",
  ) +
  theme_minimal()
```

\begin{questions}

\setcounter{question}{44}

\short Draw a scatter plot that illustrates heteroskedasticity. Label the axes.

\begin{solution}
  Several options here, but there should be a scatter plot with an explanatory variable on the x-axis and a disturbance on the y-axis. The plot should show that the variance of the disturbance changes with the value of the explanatory variable.
\end{solution}
\end{questions}

```{r, heteroskedasticity}
#| echo: false
#| fig.height: 3 

# Load packages
pacman::p_load(tibble, ggplot2, scales)
# Make data
set.seed(123)
df = tibble(
  x = runif(100, 0, 100),
  u = rnorm(100, 0, x)
)
# Plot
ggplot(df, aes(x = x, y = u)) +
  geom_point() +
  labs(
    x = 'Explanatory variable',
    y = 'Disturbance',
  ) +
  theme_minimal()
```
